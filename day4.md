# Actor-Critic框架与相关概念

## 1. Actor-Critic框架（演员-评论家框架）
想象一个演员在演戏，旁边站着一位导演。
- **演员（Actor）**：负责做动作（比如选择动作）。它的目标是通过不断尝试，学会“什么情况下该做什么动作”。
- **评论家（Critic）**：负责评价演员的动作（比如给动作打分）。它会告诉演员：“刚才的动作在那种状态下是好还是坏，下次应该改进哪里。”

### 实际原理
- **Actor** 是一个策略网络（比如神经网络），直接输出动作的概率（例如：“在红灯时，有70%概率刹车，30%概率加速”）。
- **Critic** 是一个价值网络，评估当前状态（或状态+动作）的价值（例如：“现在在高速公路上，当前状态的价值是高分”）。

### 协作方式
Actor 根据 Critic 的反馈调整自己的策略。比如，Critic 说某个动作的评分很高，Actor 就会增加选择这个动作的概率。

## 2. 优势函数（Advantage）


考试时，你考了80分，但全班平均分是60分，那么你的“优势”是20分。

### 定义
优势函数衡量的是“某个动作比平均表现好多少”。

### 公式
$$
A(s,a) = Q(s,a) - V(s)
$$
- $Q(s,a)$：在状态 $s$ 下做动作 $a$ 的预期总收益。
- $V(s)$：状态 $s$ 的平均预期收益（基线）。

### 意义
- 如果 $A(s,a) > 0$，说明动作 $a$ 比平均动作好，应该多选；
- 如果 $A(s,a) < 0$，说明动作 $a$ 比平均动作差，应该少选。

## 3. 基线（Baseline）的作用


考试时，如果题目特别难，全班平均分只有30分，你考了50分就算不错了；但如果题目简单，平均分80分，你考70分反而差。基线帮你校准“好与坏”的标准。

### 作用
基线（通常是状态价值 $V(s)$）的作用是减少方差。

### 为什么需要基线
- 直接使用动作价值 $Q(s,a)$ 的绝对值可能有偏差（比如某些环境天然奖励高）。
- 减去基线 $V(s)$ 后，优势函数 $A(s,a)$ 只关注动作的相对好坏，训练更稳定。


## 4. 自举（Bootstrap）机制及其重要性


天气预报不是完全靠卫星数据，而是结合历史数据和当前观测来预测明天天气。

### 定义
自举指用当前的估计值（比如 Critic 的预测）来更新未来的估计值。

### 在强化学习中的体现
- Critic 的更新不是完全依赖真实环境的最终回报，而是用当前 Critic 的预测值来部分替代未来回报。
- 例如，TD误差（Temporal Difference Error）公式：
  $$
  \delta = 当前奖励 + \gamma \cdot Critic预测的下个状态价值 - Critic预测的当前状态价值
  $$

### 重要性
- **加速学习**：不需要等到一个回合结束才更新，可以边行动边学习（比如下棋时每走一步就调整策略）。
- **减少方差**：用部分真实数据（当前奖励）和部分估计值（Critic预测）混合更新，比完全依赖回合结束的回报更稳定。

### 例子
如果 Critic 预测“从当前位置到终点能得100分”，但实际走了一步只得到10分，Critic 会修正预测为：
$$
新预测 = 10分 + \gamma \cdot 新位置的预测分
$$
这样逐步逼近真实值。

## 5. TD-error推导过程

### 推导过程
我们要推导的是：
$$
\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
$$

#### 步骤1：先理解什么是理想情况
想象你在学习烤蛋糕，每个步骤都有评分：
- $V(s_t)$ = 当前步骤（比如搅拌面糊）的预估总得分
- $G_t$ = 从当前步骤到烤完蛋糕的实际总得分

在理想情况下，我们希望：
$$
V(s_t) = \mathbb{E}[G_t]
$$
即：预估得分应该等于实际总得分的期望值。

#### 步骤2：建立时间关联
实际总得分可以拆分为：
$$
G_t = r_t（当前步骤得分） + \gamma G_{t+1}（下一步骤的得分，打折后）
$$
（$\gamma$是折扣因子，比如0.9，表示越后面的步骤越不确定）

#### 步骤3：引入现实限制
但现实中，我们无法提前知道$G_{t+1}$（比如面糊还没进烤箱，不知道烤出来的结果）。于是我们用当前的预估$V(s_{t+1})$来代替$G_{t+1}$：
$$
G_t \approx r_t + \gamma V(s_{t+1})
$$
这就像预测："搅拌面糊的得分=当前搅拌动作的得分 + 预估的后续步骤得分"

#### 步骤4：计算预测误差
TD-error就是实际发生的奖励与预测值的差异：
$$
\delta_t = (实际观测到的部分结果) - (原始预测) = [r_t + \gamma V(s_{t+1})] - V(s_t)
$$

### 完整推导
原始定义：
$$
V(s_t) 应该等于当前奖励 + 打折后的未来价值期望 \rightarrow V(s_t) = \mathbb{E}[r_t + \gamma V(s_{t+1})]
$$
两边同时减去$V(s_t)$：
$$
0 = \mathbb{E}[r_t + \gamma V(s_{t+1}) - V(s_t)]
$$
这个差值就是TD-error的期望：
$$
\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
$$

### 理解
就像烤蛋糕时，每完成一个步骤就立刻修正预测：
"我刚才预估搅拌面糊能带来80分，但实际搅拌得了20分，并且进入下一步（倒模具）的预估是70分。那么误差就是$20+0.9 \times 70 -80=20+63-80=3$分，说明之前低估了"

### 物理意义
- 如果$\delta_t > 0$：说明实际结果比预估好，要调高$V(s_t)$
- 如果$\delta_t < 0$：说明预估过于乐观，要降低$V(s_t)$


## 补充内容：A2C算法、SARSA算法及自举机制的符号系统完善

### 1. A2C算法（Advantage Actor-Critic）

#### 简介
A2C是Actor-Critic框架的一种改进版本，它使用优势函数（Advantage Function）来帮助Actor更好地学习策略。A2C的核心思想是通过减去基线（Baseline）来减少方差，从而加速学习。

#### 算法步骤
1. **初始化**：初始化Actor（策略网络）和Critic（价值网络）。
2. **采样**：在当前策略下，采样若干步的状态、动作、奖励和下一个状态。
3. **计算优势函数**：使用Critic网络计算每个状态的价值$V(s)$，然后计算优势函数$A(s,a)$：
   $$
   A(s,a) = Q(s,a) - V(s)
   $$
   其中，$Q(s,a)$可以通过蒙特卡洛方法（MC）或时序差分方法（TD）估计。
4. **更新Actor**：使用优势函数更新策略网络，使得优势大的动作被选择的概率增加：
   $$
   \nabla_\theta J(\theta) = \mathbb{E}[\nabla_\theta \log \pi_\theta(a|s) A(s,a)]
   $$
5. **更新Critic**：使用TD-error更新价值网络，使得Critic的预测更准确：
   $$
   \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
   $$
   $$
   \nabla_w J(w) = \mathbb{E}[\delta_t \nabla_w V_w(s_t)]
   $$
6. **重复**：重复上述步骤，直到策略收敛。

### 2. SARSA算法（State-Action-Reward-State-Action）

#### 简介
SARSA是一种基于时序差分（TD）的强化学习算法，它通过直接学习动作价值函数$Q(s,a)$来选择动作。SARSA的名字来源于其更新公式中使用的五个元素：当前状态$S_t$，当前动作$A_t$，当前奖励$R_t$，下一个状态$S_{t+1}$，以及下一个动作$A_{t+1}$。

#### 算法步骤
1. **初始化**：初始化动作价值函数$Q(s,a)$。
2. **选择动作**：在当前状态$s_t$下，根据当前策略（如$\epsilon$-贪婪策略）选择动作$a_t$。
3. **执行动作**：执行动作$a_t$，观察奖励$r_t$和下一个状态$s_{t+1}$。
4. **选择下一个动作**：在下一个状态$s_{t+1}$下，根据当前策略选择下一个动作$a_{t+1}$。
5. **更新动作价值函数**：使用SARSA更新公式更新$Q(s,a)$：
   $$
   Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]
   $$
   其中，$\alpha$是学习率，$\gamma$是折扣因子。
6. **重复**：重复上述步骤，直到动作价值函数收敛。

### 3. 自举机制的符号系统完善

#### 蒙特卡洛方法（MC）
蒙特卡洛方法通过完整的回合（Episode）来估计价值函数。它的更新公式为：
$$
V(s_t) \leftarrow V(s_t) + \alpha [G_t - V(s_t)]
$$
其中，$G_t$是从状态$s_t$开始的累计折扣回报：
$$
G_t = \sum_{k=0}^{T-t} \gamma^k r_{t+k}
$$

#### 时序差分方法（TD）
时序差分方法通过部分观测的奖励和下一个状态的估计值来更新价值函数。它的更新公式为：
$$
V(s_t) \leftarrow V(s_t) + \alpha [r_t + \gamma V(s_{t+1}) - V(s_t)]
$$
其中，$r_t + \gamma V(s_{t+1})$是TD目标，$r_t + \gamma V(s_{t+1}) - V(s_t)$是TD-error。

#### 自举机制的比较
- **MC方法**：依赖于完整的回合回报，方差较大，但偏差较小。
- **TD方法**：依赖于部分观测的奖励和下一个状态的估计值，方差较小，但可能存在偏差。

#### 自举机制的效果
- **加速学习**：TD方法可以在每个时间步进行更新，而不需要等待回合结束，从而加速学习过程。
- **减少方差**：通过使用部分真实数据和部分估计值，TD方法可以减少方差，使得学习过程更加稳定。
- **在线学习**：TD方法适用于在线学习场景，可以在与环境交互的同时进行学习和更新。