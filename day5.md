# Day 5：PPO核心理论

### 重要性采样
重要性采样是一种用于估计期望值的统计学方法，尤其在强化学习中非常有用。它的核心思想是：通过从一个分布中采样，来估计另一个分布下的期望值。

1. **为什么要用重要性采样？**
   在强化学习中，我们经常需要计算某个策略 $\pi$ 下的期望回报：
   $$\mathbb{E}_{s, a \sim \pi}[f(s, a)]$$
   但直接采样可能效率低下，比如：
   - 策略 $\pi$ 的采样成本高（例如需要与环境交互）。
   - 策略 $\pi$ 的某些区域采样概率很低，导致估计不准确。

   这时，我们可以从一个容易采样的分布 $q(s, a)$ 中采样，然后通过重要性采样来估计 $\pi$ 下的期望值。

2. **基本思想**
   假设我们想计算分布 $p(x)$ 下的期望值：
   $$\mathbb{E}_{x \sim p}[f(x)] = \int p(x) f(x) \, dx$$
   但如果直接从 $p(x)$ 采样困难，我们可以从一个提议分布 $q(x)$ 中采样，然后通过调整权重来修正偏差。

   重要性采样公式：
   $$\mathbb{E}_{x \sim p}[f(x)] = \mathbb{E}_{x \sim q}\left[\frac{p(x)}{q(x)} f(x)\right]$$
   其中：
   - $\frac{p(x)}{q(x)}$ 是重要性权重，用于修正从 $q(x)$ 采样带来的偏差。
   - $q(x)$ 需要满足 $q(x) > 0$ 当 $p(x) > 0$（即 $q(x)$ 的支撑集包含 $p(x)$ 的支撑集）。

3. **重要性采样在强化学习中的应用**
   在强化学习中，我们经常需要比较或优化两个策略 $\pi_{old}$ 和 $\pi_{new}$。假设我们想计算新策略 $\pi_{new}$ 下的期望回报，但只能从旧策略 $\pi_{old}$ 中采样。

   目标：估计新策略 $\pi_{new}$ 下的期望回报：
   $$\mathbb{E}_{s, a \sim \pi_{new}}[f(s, a)]$$
   重要性采样公式：
   $$\mathbb{E}_{s, a \sim \pi_{new}}[f(s, a)] = \mathbb{E}_{s, a \sim \pi_{old}}\left[\frac{\pi_{new}(a|s)}{\pi_{old}(a|s)} f(s, a)\right]$$
   其中：
   - $\frac{\pi_{new}(a|s)}{\pi_{old}(a|s)}$ 是重要性权重，用于修正从 $\pi_{old}$ 采样带来的偏差。

4. **直观理解**
   - 权重的作用：如果某个动作 $a$ 在新策略 $\pi_{new}$ 下的概率比旧策略 $\pi_{old}$ 高，那么它的权重 $\frac{\pi_{new}(a|s)}{\pi_{old}(a|s)}$ 会大于1，放大其贡献；反之，权重会小于1，减小其贡献。
   - 修正偏差：通过权重调整，我们可以用旧策略的采样数据来准确估计新策略的期望值。

5. **优缺点**
   - 优点：
     - 可以从一个容易采样的分布中估计另一个分布的期望值。
     - 在强化学习中，可以利用旧策略的数据来评估或优化新策略，减少与环境交互的成本。
   - 缺点：
     - 如果 $q(x)$ 和 $p(x)$ 差异很大，重要性权重 $\frac{p(x)}{q(x)}$ 可能会非常大或非常小，导致估计的方差很大。
     - 需要确保 $q(x)$ 的支撑集包含 $p(x)$ 的支撑集，否则某些区域的样本会被忽略。

#### **应用分析**
假设我们正在训练一个无人机控制模型，目标是让无人机在复杂环境中（例如城市街道或森林）自主飞行并完成任务。我们使用强化学习来优化无人机的控制策略。

##### 场景描述
- **任务目标**：无人机需要从起点飞到终点，同时避开障碍物。
- **策略**：我们有两个策略：
  - 旧策略 $\pi_{old}$：一个基于简单规则的控制策略（例如直线飞行，遇到障碍物时随机转向）。
  - 新策略 $\pi_{new}$：一个基于深度神经网络的高级控制策略，能够更智能地规划路径。

##### 问题
我们想评估新策略 $\pi_{new}$ 的性能，但由于以下原因，直接从 $\pi_{new}$ 采样数据可能效率低下：
- **采样成本高**：每次与环境交互（即无人机实际飞行）都需要时间和资源。
- **探索不足**：新策略 $\pi_{new}$ 可能在某些区域（例如复杂障碍物区域）采样概率很低，导致估计不准确。

因此，我们希望利用旧策略 $\pi_{old}$ 的采样数据来估计新策略 $\pi_{new}$ 的期望回报。

##### 重要性采样的应用
目标：估计新策略 $\pi_{new}$ 下的期望回报：
$$\mathbb{E}_{s, a \sim \pi_{new}}[f(s, a)]$$
其中 $f(s, a)$ 是无人机在状态 $s$ 下执行动作 $a$ 后获得的回报（例如飞行距离、避障成功与否等）。

重要性采样公式：
由于我们只能从旧策略 $\pi_{old}$ 中采样，因此使用重要性采样公式：
$$\mathbb{E}_{s, a \sim \pi_{new}}[f(s, a)] = \mathbb{E}_{s, a \sim \pi_{old}}\left[\frac{\pi_{new}(a|s)}{\pi_{old}(a|s)} f(s, a)\right]$$

##### 具体步骤
1. **采样**：使用旧策略 $\pi_{old}$ 控制无人机飞行，收集一系列状态-动作对 $(s, a)$ 和对应的回报 $f(s, a)$。
2. **计算重要性权重**：对于每个采样到的 $(s, a)$，计算重要性权重 $\frac{\pi_{new}(a|s)}{\pi_{old}(a|s)}$。
   - 如果新策略 $\pi_{new}$ 在状态 $s$ 下选择动作 $a$ 的概率比旧策略 $\pi_{old}$ 高，权重会大于1，放大其贡献。
   - 如果新策略 $\pi_{new}$ 在状态 $s$ 下选择动作 $a$ 的概率比旧策略 $\pi_{old}$ 低，权重会小于1，减小其贡献。
3. **估计期望回报**：对所有采样数据加权平均，得到新策略 $\pi_{new}$ 下的期望回报估计。

##### 例子
假设在某个状态 $s$（例如无人机前方有一个障碍物），旧策略 $\pi_{old}$ 选择动作 $a_1$（左转）的概率是 0.5，选择动作 $a_2$（右转）的概率是 0.5。新策略 $\pi_{new}$ 在相同状态 $s$ 下选择动作 $a_1$ 的概率是 0.8，选择动作 $a_2$ 的概率是 0.2。
- 如果采样到 $(s, a_1)$，其重要性权重为 $\frac{0.8}{0.5} = 1.6$。
- 如果采样到 $(s, a_2)$，其重要性权重为 $\frac{0.2}{0.5} = 0.4$。

最终，新策略 $\pi_{new}$ 的期望回报估计会更多地依赖于动作 $a_1$ 的贡献，因为其权重更大。

##### 优点与挑战
- **优点**：
  - 可以利用旧策略 $\pi_{old}$ 的采样数据来评估新策略 $\pi_{new}$，减少实际飞行次数。
  - 在复杂环境中，旧策略 $\pi_{old}$ 可能更容易探索到某些关键区域（例如障碍物附近），从而帮助新策略 $\pi_{new}$ 更好地学习。
- **挑战**：
  - 如果新策略 $\pi_{new}$ 和旧策略 $\pi_{old}$ 差异很大，重要性权重可能会非常大或非常小，导致估计的方差很大。
  - 需要确保旧策略 $\pi_{old}$ 的采样覆盖了新策略 $\pi_{new}$ 可能访问的所有状态-动作对。

通过重要性采样，我们可以在无人机控制模型的训练中更高效地利用历史数据，加速策略优化过程。

### TRPO（Trust Region Policy Optimization）的引入

#### 理论背景
在策略优化中，直接更新策略可能会导致性能的剧烈波动，甚至崩溃。TRPO 的核心思想是：在每次策略更新时，限制新策略与旧策略之间的差异，确保更新在一个“信任区域”内进行，从而保证策略改进的稳定性。

TRPO 的数学目标是：
$$
\max_\theta \mathbb{E}_{s, a \sim \pi_{old}} \left[ \frac{\pi_\theta(a|s)}{\pi_{old}(a|s)} A^{\pi_{old}}(s, a) \right]
$$
其中：
- $\pi_\theta$ 是新策略（参数为 $\theta$）。
- $\pi_{old}$ 是旧策略。
- $A^{\pi_{old}}(s, a)$ 是优势函数，表示在状态 $s$ 下执行动作 $a$ 的相对价值。
- $\frac{\pi_\theta(a|s)}{\pi_{old}(a|s)}$ 是重要性权重。

约束条件：
$$
\mathbb{E}_{s \sim \pi_{old}} \left[ D_{KL}(\pi_{old}(\cdot|s) \parallel \pi_\theta(\cdot|s)) \right] \leq \delta
$$
其中 $D_{KL}$ 是 KL 散度，用于衡量新旧策略之间的差异，$\delta$ 是一个小的阈值。

#### 执行步骤
假设我们正在训练无人机的控制策略：
- 旧策略 $\pi_{old}$：无人机在遇到障碍物时随机选择左转或右转。
- 新策略 $\pi_\theta$：我们希望优化策略，使无人机更倾向于选择避开障碍物的方向。

使用 TRPO 时：
1. 从旧策略 $\pi_{old}$ 中采样数据（无人机的飞行轨迹）。
2. 计算优势函数 $A^{\pi_{old}}(s, a)$，例如：
   - 如果无人机成功避开障碍物，优势值为正。
   - 如果无人机撞到障碍物，优势值为负。
3. 优化新策略 $\pi_\theta$，但限制其与旧策略 $\pi_{old}$ 的 KL 散度不超过阈值 $\delta$，确保更新是稳定的。

### PPO（Proximal Policy Optimization）的引入

#### 理论背景
TRPO 虽然有效，但其实现复杂（需要计算 KL 散度并求解约束优化问题）。PPO 是对 TRPO 的简化，通过裁剪重要性权重来限制策略更新的幅度，从而避免性能崩溃。

PPO 的目标函数为：
$$
L^{CLIP}(\theta) = \mathbb{E}_{s, a \sim \pi_{old}} \left[ \min \left( r(\theta) A^{\pi_{old}}(s, a), \text{clip}(r(\theta), 1-\epsilon, 1+\epsilon) A^{\pi_{old}}(s, a) \right) \right]
$$
其中：
- $r(\theta) = \frac{\pi_\theta(a|s)}{\pi_{old}(a|s)}$ 是重要性权重。
- $\text{clip}(r(\theta), 1-\epsilon, 1+\epsilon)$ 将重要性权重限制在 $[1-\epsilon, 1+\epsilon]$ 范围内。
- $\epsilon$ 是一个超参数（通常取 0.1 或 0.2）。

#### 执行步骤
1. 从旧策略 $\pi_{old}$ 中采样数据。
2. 计算重要性权重 $r(\theta) = \frac{\pi_\theta(a|s)}{\pi_{old}(a|s)}$。
3. 对重要性权重进行裁剪：
   - 如果 $r(\theta) > 1+\epsilon$，将其裁剪为 $1+\epsilon$。
   - 如果 $r(\theta) < 1-\epsilon$，将其裁剪为 $1-\epsilon$。
4. 优化目标函数 $L^{CLIP}(\theta)$，确保策略更新是稳定的。

### PPO Clip 的实现

#### 理论背景
PPO Clip 是 PPO 的一种具体实现方式，通过裁剪重要性权重来限制策略更新的幅度。其核心思想是：
- 如果新策略与旧策略差异过大，裁剪重要性权重，避免更新过快。
- 如果新策略与旧策略差异较小，直接使用重要性权重进行更新。

#### 执行步骤
假设在某个状态 $s$（无人机前方有障碍物）：
- 旧策略 $\pi_{old}$ 选择左转的概率为 0.5，选择右转的概率为 0.5。
- 新策略 $\pi_\theta$ 选择左转的概率为 0.8，选择右转的概率为 0.2。

计算重要性权重：
- 对于左转：$r(\theta) = \frac{0.8}{0.5} = 1.6$。
- 对于右转：$r(\theta) = \frac{0.2}{0.5} = 0.4$。

假设 $\epsilon = 0.2$，则裁剪后的重要性权重为：
- 对于左转：$\text{clip}(1.6, 0.8, 1.2) = 1.2$。
- 对于右转：$\text{clip}(0.4, 0.8, 1.2) = 0.8$。

最终，PPO Clip 的目标函数会使用裁剪后的重要性权重来计算策略梯度，确保更新是稳定的。

---

### PPO推导过程

PPO（Proximal Policy Optimization）的推导过程结合了策略梯度方法、重要性采样和KL散度约束，旨在稳定策略更新并提高样本效率。以下是其公式的逐步推导：

#### 1. 策略梯度与替代目标函数
策略梯度方法的目标是最大化期望回报，其梯度为：
$$
\nabla J(\theta) = \mathbb{E}_{s, a \sim \pi_\theta}[\nabla \log \pi_\theta(a|s) \cdot A^{\pi_\theta}(s, a)],
$$
其中 $A^{\pi_\theta}(s, a)$ 是优势函数。为提升样本效率，重要性采样被引入，允许使用旧策略 $\pi_{\theta_k}$ 生成的样本优化新策略 $\pi_\theta$。此时，替代目标函数为：
$$
J_{\theta_k}(\theta) = \mathbb{E}_{s, a \sim \pi_{\theta_k}} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)} A^{\pi_{\theta_k}}(s, a) \right].
$$
此目标函数通过重要性采样比率 $\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}$ 衡量新旧策略差异，但其直接优化可能导致策略更新过大。

#### 2. KL散度约束
为防止策略更新偏离旧策略过远，TRPO（Trust Region Policy Optimization）提出约束新旧策略的KL散度：
$$
\max_\theta J_{\theta_k}(\theta) \quad \text{s.t.} \quad KL(\pi_{\theta_k}, \pi_\theta) \leq \delta.
$$
其中 $KL(\pi_{\theta_k}, \pi_\theta) = \mathbb{E}_s[KL(\pi_{\theta_k}(\cdot|s) \parallel \pi_\theta(\cdot|s))]$ 为平均KL散度。TRPO通过二阶优化方法求解，但计算复杂。

#### 3. 无约束优化与KL惩罚项
PPO将TRPO的约束优化转化为无约束优化问题，引入KL散度作为惩罚项：
$$
J^{PPO}_{\theta_k}(\theta) = J_{\theta_k}(\theta) - \beta \cdot KL(\pi_{\theta_k}, \pi_\theta).
$$
其中 $\beta$ 是惩罚权重。通过调整 $\beta$，可间接控制KL散度的上限，替代TRPO的硬约束。

#### 4. 自适应调整惩罚权重 $\beta$
为使KL散度保持在合理范围内，PPO采用自适应调整策略：
- 若 $KL(\pi_{\theta_k}, \pi_\theta) > KL_{max}$，增大 $\beta$（如 $\beta \leftarrow 2\beta$）。
- 若 $KL(\pi_{\theta_k}, \pi_\theta) < KL_{min}$，减小 $\beta$（如 $\beta \leftarrow \beta/2$）。

通过动态调节 $\beta$，确保策略更新幅度稳定，避免性能崩溃。

#### 5. 最终PPO目标函数
综合上述推导，PPO的优化目标为：
$$
J^{PPO}_{\theta_k}(\theta) = \mathbb{E}_{s, a \sim \pi_{\theta_k}} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)} A^{\pi_{\theta_k}}(s, a) \right] - \beta \cdot KL(\pi_{\theta_k}, \pi_\theta).
$$

#### 总结
- **替代目标函数**：利用重要性采样复用旧策略样本，避免重复采样。
- **KL惩罚项**：限制策略更新幅度，替代TRPO的复杂约束。
- **自适应调整 $\beta$**：动态平衡策略改进与稳定性，无需手动调参。
TRPO使用硬约束（KL散度约束）来限制策略更新，需要通过二阶优化方法（如共轭梯度法）求解，计算复杂度高。  
PPO将KL散度作为惩罚项引入目标函数，转化为无约束优化问题，可以直接使用一阶优化方法（如SGD或Adam）求解，计算效率更高。

### PPO优势分析
- **计算友好**：仅需一阶优化（Adam即可）
- **实现简单**：无需计算KL散度或共轭梯度
- **数据高效**：支持多轮次策略更新（传统方法需重采样）
- **空间限制**：在旧策略的信任域内寻找改进方向
- **时间限制**：通过clip机制确保新旧策略差异不超过阈值
- 类似"摸着石头过河"：每一步更新都紧贴着老策略的"记忆"


### Clip参数ε的影响
- **ε较小**（如0.1）：更新保守，训练稳定但收敛慢
- **ε较大**（如0.3）：允许更大更新，可能震荡
- **经验值**：连续控制任务常用0.2，Atari游戏用0.1

